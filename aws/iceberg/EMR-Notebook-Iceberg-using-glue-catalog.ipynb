{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Example Using Glue Catalog Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics covered in this example\n",
    "\n",
    "1) [Configuring Iceberg](#configure_iceberg) <br>\n",
    "2) [Creating an Iceberg Table](#create_table) <br>\n",
    "3) [DML Statements](#dml) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [Inserts](#inserts) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;b) [Deletes](#deletes) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;d) [Updates](#updates) <br>\n",
    "4) [Schema Evolution](#schema_evolution) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [Adding Columns](#adding_columns) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;c) [Dropping Columns](#dropping_columns) <br>\n",
    "5) [Time Travel](#time_travel) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [Rollback](#rollback) <br>\n",
    "6) [Partition Evolution](#partition_evolution) <br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Apache Iceberg (https://iceberg.apache.org/) is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink and Hive using a high-performance table format that works just like a SQL table. Iceberg tracks individual data files in a table instead of directories. This allows writers to create data files in-place and only adds files to the table in an explicit commit. Every time a new file is inserted to any partition in this table, a new point-in-time snapshot of all the files get created. At the query time, there is no need to list a directory to find the files we need to work with, as the snapshot already has that information pre-populated during the write time (commonly known as snapshot isolation (https://en.wikipedia.org/wiki/Snapshot_isolation) in databases).\n",
    "\n",
    "Iceberg supports write, delete, update, and time travel operations with complete support for ACID transactions (https://en.wikipedia.org/wiki/ACID). Table changes are atomic and readers never see partial or uncommitted changes (serializable isolation (https://en.wikipedia.org/wiki/Isolation_(database_systems)#Serializable))\n",
    "\n",
    "Iceberg table format is an open specification at multiple levels. At the catalog level, you can plugin multiple types of catalogs such as hive, hadoop, AWS Glue Data Catalog etc. All these can co-exist. You can join tables across different types of catalogs. In this example, we are going to work with Glue Data Catalog.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Create an S3 bucket location to save sample dataset. Update the <span style=\"color:red\">MYBUCKET</span> with the bucket which you created in the Prerequisite - Create S3 Bucket section of workshop\n",
    "\n",
    "In this example we use the path format: s3://<span style=\"color:red\">MYBUCKET</span>/<span style=\"color:red\">YOUR-CATALOG-NAME</span>/tables/ \n",
    "    \n",
    "    For example: s3://MYBUCKET/glue_catalog/products.db/amazonreviews\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"configure_iceberg\"></a>\n",
    "## Configuring Iceberg on Spark session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your Spark session using the %%configure magic command. We will be using Glue Catalog for Iceberg Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Update the <span style=\"color:red\">MYBUCKET</span> in configuration below with you S3 bucket name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "\"conf\":{\n",
    "         \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "         \"spark.sql.catalog.glue_catalog\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "         \"spark.sql.catalog.glue_catalog.catalog-impl\":\"org.apache.iceberg.aws.glue.GlueCatalog\",\n",
    "         \"spark.sql.catalog.glue_catalog.warehouse\":\"s3://MYBUCKET/glue_catalog/\",\n",
    "         \"spark.sql.catalog.glue_catalog.io-impl\":\"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "         \"spark.sql.catalog.glue_catalog.lock-impl\":\"org.apache.iceberg.aws.glue.DynamoLockManager\",\n",
    "         \"spark.sql.catalog.glue_catalog.lock.table\":\"myGlueLockTable\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"catalogs\"></a>\n",
    "## Iceberg Catalogs and Namespaces\n",
    "The default catalog is the `AwsDataCatalog`. Let us switch to our Glue catalog -  `glue_catalog` that has support for Iceberg tables. Note that there are no namespaces. A namespace in iceberg is the same thing as a database in Glue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use glue_catalog\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS products\")\n",
    "spark.sql(\"use products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_table\"></a>\n",
    "## Creating an Iceberg Table\n",
    "\n",
    "Create Iceberg Table, this table is using Glue Catalog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" DROP TABLE if exists glue_catalog.products.amazonreviews\"\"\")\n",
    "\n",
    "spark.sql(\"\"\" CREATE TABLE glue_catalog.products.amazonreviews \n",
    "(marketplace\tstring\n",
    ",customer_id\tstring\n",
    ",review_id\tstring\n",
    ",product_category\tstring\n",
    ",product_id\tstring\n",
    ",product_parent\tstring\n",
    ",product_title\tstring\n",
    ",star_rating\tint\n",
    ",helpful_votes\tint\n",
    ",total_votes\tint\n",
    ",vine\tstring\n",
    ",verified_purchase\tstring\n",
    ",review_headline\tstring\n",
    ",review_body\tstring\n",
    ",review_date\tbigint)\n",
    "\n",
    "USING iceberg \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test the tables created in glue catalog.\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dml\"></a>\n",
    "## DML Operations\n",
    "Icerberg supports all DML statements to add or modify data in your data lake: Inserts to add new data, Updates to modify specific columns in specific rows in your existing data, Deletes for GDPR and CCPA compliance and Upserts when you have incoming data that may have a mix of inserts and updates. Let us look at each of them now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inserts\"></a>\n",
    "### Inserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will be using simulated Amazon Product Reviews  dataset.**\n",
    "\n",
    "We are loading just one partition for sake of simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\n",
    "    \"s3://MYBUCKET/productreviews/simulatedproductreviews.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell to write data into the Iceberg table, We are writing just one partition for sake of simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sortWithinPartitions(\"review_date\").writeTo(\n",
    "    \"glue_catalog.products.amazonreviews\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify data is loaded into iceberg table successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from glue_catalog.products.amazonreviews limit 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deletes\"></a>\n",
    "### Deletes\n",
    "GDPR and CCPA regulations mandate timely removal of individual customer data and other records from datasets. Iceberg is designed to be able to handle these trivially.\n",
    "Now let us delete a record from our Iceberg table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all records from the table for verified_purchase = 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"delete from glue_catalog.products.amazonreviews\n",
    "where verified_purchase = 'N'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if data is deleted. Below query should produce zero records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"select * from glue_catalog.products.amazonreviews where verified_purchase = 'N'limit 9\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"updates\"></a>\n",
    "### Updates\n",
    "What if we want to go back and update an existing record? Let's change the `marketplace` from US to USA. Iceberg allows updates using a simple `UPDATE` and`SET` clause added to your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"UPDATE glue_catalog.products.amazonreviews\n",
    "SET marketplace = 'USA'\n",
    "WHERE marketplace = 'US'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify 'marketplace' column is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from glue_catalog.products.amazonreviews limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"schema_evolution\"></a>\n",
    "## Schema Evolution\n",
    "Borrowing from the way columns work in databases, Iceberg tracks columns by using unique IDs and not by the column name. As long as the ID is the same, all the data still remains. You can safely add, drop, rename, update, or even reorder columns. You don’t have to rewrite the data for this. Schema evolution gets first class citizen treatment in Iceberg. Your ingest and read queries now have the freedom to be evolved without having to hide the schema inside JSON blobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will add a column to the iceberg table which we just created. We will add comment column to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"adding_columns\"></a>\n",
    "### Adding Columns\n",
    "Now we are going to add another column called `high_rated_product`. Iceberg also allows documenting the purpose for each column as `comment`, which helps a lot in a collaborative environment and quick lookup of data from business users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"ALTER TABLE glue_catalog.products.amazonreviews ADD COLUMNS (high_rated_product string comment 'Highly rated comment')\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add **High rated** flag to the comment column where rating is greater or equal to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"UPDATE glue_catalog.products.amazonreviews SET high_rated_product = 'High rated' where star_rating >=4\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify column is added successfully by quering the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "Select customer_id,review_id,product_id, product_title, star_rating, high_rated_product from glue_catalog.products.amazonreviews limit 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dropping_columns\"></a>\n",
    "### Dropping Columns\n",
    "Now, there is a change in business requirements, we are not interested in the `high_rated_product` column anymore and need to remove that column from our table. Iceberg allows us to do that easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"ALTER TABLE glue_catalog.products.amazonreviews DROP COLUMN high_rated_product\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"time_travel\"></a>\n",
    "## Time Travel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iceberg does give us a way to look at the history of changes to our table using the `history` metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM glue_catalog.products.amazonreviews.history\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Iceberg does give us a way to look at the snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM glue_catalog.products.amazonreviews.snapshots\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rollback\"></a>\n",
    "### Rollback\n",
    "To undo the recent changes, we can execute Iceberg stored procedures using `CALL` statement to rollback the state of the table to any historical commit using `rollback_to_snapshot` stored procedure. We could also use `rollback_to_timestamp`.\n",
    "\n",
    "Recover the table to its original state, replace the <span style=\"color:red\">xxxxxxxxxxxxx</span> with Snapshot id from Table History. Use the snapshot_id with parent_id = null from Table History (first record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"CALL glue_catalog.system.rollback_to_snapshot('products.amazonreviews', xxxxxxxxxxxxx)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our table is now back to original state. You can verifiy this by observing verified_purchase column it shoould show both 'Y' and 'N' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from glue_catalog.products.amazonreviews limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"partition_evolution\"></a>\n",
    "##  Partition Evolution\n",
    "Let us look at the partitions we have in our table by querying the partitions metadata table. Iceberg keeps track of how many records (record_count column) and how many files (file_count column) are present in each partition. This is a very handy tool that could be used for performance and data quality related troubleshooting and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from glue_catalog.products.amazonreviews.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us list our s3 bucket location to see the partitions. Currently our table has no partitioning. All data files are just at the root data prefix. (Remember to replace <span style=\"color:red\">MYBUCKET</span> with your bucket name and if you use different prefixes, update the path as applicable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://MYBUCKET/glue_catalog/products.db/amazonreviews/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-09T00:46:24.009559Z",
     "iopub.status.busy": "2023-12-09T00:46:24.009331Z",
     "iopub.status.idle": "2023-12-09T00:46:24.077801Z",
     "shell.execute_reply": "2023-12-09T00:46:24.076930Z",
     "shell.execute_reply.started": "2023-12-09T00:46:24.009535Z"
    }
   },
   "source": [
    "<a id=\"Create new column in Date format\"></a>\n",
    "###  Create new column with Date datatype\n",
    "Let's create a new column **review_dt**.  \n",
    "We will use this new column for our new partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE glue_catalog.products.amazonreviews ADD COLUMNS (review_dt date);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the new column with date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "UPDATE glue_catalog.products.amazonreviews set review_dt = date_add(to_date('1970-01-01'),cast(review_date as integer));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first create **yearly** partitions. Iceberg allows us to add partitions without having to perform any data movement or any additional changes to the underlying data. `ADD PARTITION FIELD` is a simple metadata operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE glue_catalog.products.amazonreviews add PARTITION FIELD years(review_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to use the old partition on the old data. There is no change to the underlying partition structure on existing data as shown below (Again remember to replace <span style=\"color:red\">MYBUCKET</span> with your bucket name and if you use different prefixes, update the path as applicable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://MYBUCKET/glue_catalog/products.db/amazonreviews/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's insert new data for years 1998 and 2015. This will create duplicate rows, but we're just testing what happens if new data is inserted after new partition is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO glue_catalog.products.amazonreviews\n",
    "SELECT * FROM glue_catalog.products.amazonreviews WHERE year(review_dt)=1998 \n",
    "union \n",
    "SELECT * FROM glue_catalog.products.amazonreviews WHERE year(review_dt)=2015 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the S3 bucket structure again.  You should now see new directories based on the new partitions.  \n",
    "Before running the following cell, replace <span style=\"color:red\">MYBUCKET</span>  with your bucket name and if you use different prefixes, update the path as applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://MYBUCKET/glue_catalog/products.db/amazonreviews/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume down the time line, we realize we need to add **monthly** partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE glue_catalog.products.amazonreviews ADD PARTITION FIELD months(review_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's insert new data after the month partition has been set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO glue_catalog.products.amazonreviews\n",
    "SELECT * FROM glue_catalog.products.amazonreviews WHERE year(review_dt)=1998 and month(review_dt)=9\n",
    "union \n",
    "SELECT * FROM glue_catalog.products.amazonreviews WHERE year(review_dt)=2000 and month(review_dt)=9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the results with new data with month partitions. Iceberg adds the new Monthly partitions under the year partitions under which we inserted our new records.   (Replace <span style=\"color:red\">MYBUCKET</span> with your bucket name and if you use different prefixes, update the path as applicable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Top level ------------------------------------------------------------------------------------\"\n",
    "aws s3 ls s3://MYBUCKET/glue_catalog/products.db/amazonreviews/data/\n",
    "echo \"\"\n",
    "echo \"inside of review_dt_year=1998 directory-------------------------------------------------------\"\n",
    "aws s3 ls s3://MYBUCKET/glue_catalog/products.db/amazonreviews/data/review_dt_year=1998/\n",
    "echo \"\"\n",
    "echo \"inside of review_dt_year=2000 directory-------------------------------------------------------\"\n",
    "aws s3 ls s3://MYBUCKET/glue_catalog/products.db/amazonreviews/data/review_dt_year=2000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us query our table using the new monthly partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT marketplace, product_title, review_dt FROM glue_catalog.products.amazonreviews WHERE year(review_dt)=1998 and month(review_dt)=9 limit 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to query our old data with using the year() transform. There is only the original review_dt column in the table. We don't have to store additional columns to accommodate multiple paritioning schemes. Everything is in the metadata giving us immense flexibility and making our data lake forward looking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "    select marketplace, customer_id, product_category, product_title, star_rating, verified_purchase, review_headline, review_dt\n",
    "    from glue_catalog.products.amazonreviews where year(review_dt) = 1999 limit 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THANK YOU! ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
